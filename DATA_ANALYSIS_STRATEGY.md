# Data Analysis Strategy
**Date**: 2025-11-27
**Status**: Foundational analysis framework established

---

## Overview

Data analysis in the PoF pipeline is now structured in THREE LAYERS, each with distinct purposes:

```
Layer 1: INPUT DATA ANALYSIS (NEW)
‚îú‚îÄ Script: 00_input_data_source_analysis.py
‚îú‚îÄ Purpose: Pre-pipeline validation
‚îú‚îÄ Focus: Raw data structure, types, formats, quality
‚îî‚îÄ When: RUN FIRST - before any processing

Layer 2: DATA PROFILING
‚îú‚îÄ Script: 01_data_profiling.py
‚îú‚îÄ Purpose: Quality assessment after loading
‚îú‚îÄ Focus: Completeness, coverage, missing patterns
‚îî‚îÄ When: After confirming input file is valid

Layer 3: EDA & FEATURE ANALYSIS (Downstream)
‚îú‚îÄ Script: 04_eda.py (after feature engineering)
‚îú‚îÄ Purpose: Feature insights and patterns
‚îú‚îÄ Focus: Feature distributions, correlations, insights
‚îî‚îÄ When: After feature engineering complete
```

---

## Layer 1: Input Data Source Analysis (NEW) ‚ú®

### Script: `00_input_data_source_analysis.py`

**Purpose**: Validate raw input data BEFORE any processing

**What It Does**:
```
Step 1: File Information
   ‚îú‚îÄ Check file exists and is readable
   ‚îú‚îÄ File size, format, last modified
   ‚îî‚îÄ Absolute path verification

Step 2: Excel Sheet Inventory
   ‚îú‚îÄ List all sheets in workbook
   ‚îú‚îÄ Identify main data sheet
   ‚îî‚îÄ Show sheet names

Step 3: Data Loading
   ‚îú‚îÄ Load data from main sheet
   ‚îú‚îÄ Report rows/columns loaded
   ‚îî‚îÄ Detect encoding issues early

Step 4: Column Inventory
   ‚îú‚îÄ All columns with data types
   ‚îú‚îÄ Unique value counts
   ‚îú‚îÄ Missing value counts
   ‚îî‚îÄ Sample values preview

Step 5: Data Type Summary
   ‚îú‚îÄ Object/String columns
   ‚îú‚îÄ Numeric columns
   ‚îú‚îÄ DateTime columns
   ‚îî‚îÄ Type distribution

Step 6: Missing Data Analysis
   ‚îú‚îÄ Columns with missing values
   ‚îú‚îÄ Missing percentage per column
   ‚îú‚îÄ Identify >50% missing columns
   ‚îî‚îÄ Recommendations for handling

Step 7: Key Column Validation
   ‚îú‚îÄ Equipment ID (cbs_id)
   ‚îú‚îÄ Fault timestamp (started at)
   ‚îú‚îÄ Grid connection date
   ‚îú‚îÄ Equipment type (≈ûebeke Unsuru)
   ‚îî‚îÄ Verify each exists and has data

Step 8: Temporal Coverage
   ‚îú‚îÄ Earliest/latest fault dates
   ‚îú‚îÄ Date range and span
   ‚îú‚îÄ Pre/post cutoff split
   ‚îî‚îÄ Check for 24M window data

Step 9: Equipment ID Analysis
   ‚îú‚îÄ Unique equipment count
   ‚îú‚îÄ Missing equipment IDs
   ‚îú‚îÄ Faults per equipment distribution
   ‚îú‚îÄ Top equipment by fault count
   ‚îî‚îÄ Validate equipment ID completeness

Step 10: Data Quality Assessment
   ‚îú‚îÄ Empty column detection
   ‚îú‚îÄ Key columns present check
   ‚îú‚îÄ Sufficient data volume check
   ‚îú‚îÄ Duplicate row detection
   ‚îî‚îÄ Quality score generation

Step 11: Data Dictionary
   ‚îú‚îÄ Sample values from each column
   ‚îú‚îÄ Data preview for validation
   ‚îî‚îÄ Manual inspection capability

Step 12: Recommendations
   ‚îú‚îÄ High priority issues
   ‚îú‚îÄ Medium priority improvements
   ‚îú‚îÄ Data handling recommendations
   ‚îî‚îÄ Next steps guidance
```

### Why It's Critical

**Before Phase 1 Fixes Testing**:
- ‚úÖ Confirms input file exists (no "File Not Found" surprises)
- ‚úÖ Validates column names match expectations
- ‚úÖ Detects encoding issues (Turkish characters)
- ‚úÖ Checks data types are correct
- ‚úÖ Verifies temporal coverage sufficient for 3/6/12M windows
- ‚úÖ Identifies missing data issues early

**Before Full Pipeline Run**:
- ‚úÖ Avoids wasted computation on invalid input
- ‚úÖ Catches structural data problems immediately
- ‚úÖ Generates baseline understanding
- ‚úÖ Documents input assumptions

### Example Output

```
INPUT DATA SOURCE ANALYSIS
==========================

STEP 1: INPUT FILE INFORMATION
‚úì File Found: combined_data_son.xlsx
  Size: 245.5 MB
  Path: /home/user/PoF2/data/combined_data_son.xlsx

STEP 2: EXCEL SHEET INVENTORY
üìã Available Sheets (1):
   1. Arƒ±zalar

STEP 3: DATA LOADING
‚úì Data loaded successfully!
   Rows: 187,234
   Columns: 24
   Total cells: 4,493,616

STEP 4: COLUMN INVENTORY
 1. cbs_id                  Type: int64          Unique:  3,165  Missing:    0
 2. started at              Type: object         Unique: 98,234  Missing:    0
 3. ≈ûebeke Unsuru           Type: object         Unique:    18   Missing:    0
 ... (18 more columns)

STEP 8: TEMPORAL COVERAGE
üìÖ Fault Date Range:
   Earliest fault: 2019-01-15
   Latest fault:   2025-07-30
   Date range:     2,382 days (6.5 years)
   Cutoff date:    2024-06-25

üìä Data Split by Cutoff:
   Before cutoff:  165,789 faults (88.5%)
   After cutoff:   21,445 faults (11.5%)

STEP 10: DATA QUALITY ASSESSMENT
‚úì No empty columns
‚úì Key columns present (cbs_id, started at)
‚úì Sufficient data (187,234 rows)
‚úì No complete duplicates

‚úÖ DATA READY FOR PROCESSING
```

---

## Layer 2: Data Profiling

### Script: `01_data_profiling.py`

**Purpose**: Validate data quality and strategy after loading

**Focus Areas**:
- Equipment ID coverage (cbs_id: 79.9% - accept 20% loss)
- Equipment type source validation (use ≈ûebeke Unsuru only)
- Age source validation (Sebekeye_Baglanma_Tarihi)
- Temporal coverage completeness
- Customer impact availability
- Date parsing and consistency

**Output**: Quality assessment report

---

## Layer 3: Exploratory Data Analysis

### Script: `04_eda.py`

**Purpose**: Analyze engineered features and patterns

**Focus Areas**:
- Feature distributions
- Failure behavior patterns
- Geographic insights
- Equipment class analysis
- Age vs failure correlation
- Customer impact effects
- Visualizations and plots

**Input**: Features after engineering
**Output**: Visualizations, insights, reports

---

## Data Analysis Workflow

### Recommended Sequence

```
PHASE: PRE-PIPELINE VALIDATION
1. Run 00_input_data_source_analysis.py
   ‚îî‚îÄ Validate: File exists, columns present, data types correct

2. Review recommendations from script 00
   ‚îî‚îÄ Address: Any critical data issues

3. Run 01_data_profiling.py
   ‚îî‚îÄ Validate: Data quality and completeness

PHASE: PIPELINE EXECUTION
4. Run 02_data_transformation.py (with Phase 1 fixes)
   ‚îî‚îÄ Validate: Equipment ID consistency

5. Run 03_feature_engineering.py
   ‚îî‚îÄ Monitor: Missing value analysis (Phase 1.5)

6. Run 04_feature_selection.py
   ‚îî‚îÄ Verify: Leakage detection working (Phase 1.3)

PHASE: MODEL TRAINING
7. Run 05-10 (model training scripts)
   ‚îî‚îÄ Monitor: Model metrics and performance

PHASE: POST-TRAINING ANALYSIS
8. Run 04_eda.py (advanced analysis)
   ‚îî‚îÄ Analyze: Feature patterns and insights
```

---

## Analysis Checklist Before Running Pipeline

### ‚úÖ Pre-Pipeline Data Validation

Run `00_input_data_source_analysis.py` and verify:

- [ ] Input file exists and readable
- [ ] File size reasonable (>100MB expected)
- [ ] Main data sheet identified correctly
- [ ] Expected columns present:
  - [ ] cbs_id (equipment ID)
  - [ ] started at (fault timestamp)
  - [ ] ≈ûebeke Unsuru (equipment type)
  - [ ] Sebekeye_Baglanma_Tarihi (grid connection date)
- [ ] Data types correct:
  - [ ] cbs_id: numeric
  - [ ] started at: datetime-compatible
  - [ ] Text columns: proper encoding
- [ ] Sufficient data:
  - [ ] >100,000 rows (we have ~187k)
  - [ ] >1,000 unique equipment
  - [ ] Data spans >2 years
- [ ] Temporal coverage adequate:
  - [ ] Data before cutoff (2024-06-25): >80%
  - [ ] Data after cutoff: >10% (for target creation)
  - [ ] >100 faults per prediction horizon
- [ ] No critical missing data:
  - [ ] cbs_id: <1% missing
  - [ ] started at: 0% missing
  - [ ] No >90% missing columns
- [ ] Key quality checks pass:
  - [ ] No empty columns
  - [ ] Reasonable duplicate count
  - [ ] No encoding errors
- [ ] No obvious data corruption:
  - [ ] Dates are valid
  - [ ] Equipment IDs are numeric
  - [ ] Text columns readable

---

## Phase 1.5 Integration

The input data analysis directly supports **Phase 1.5** (Standardized Imputation):

```
Input Data Analysis (Script 00)
           ‚Üì
Identifies columns with missing values
           ‚Üì
Phase 1.5 (Script 03)
Analyzes high-missing features
           ‚Üì
Recommendations for exclusion/imputation
           ‚Üì
Downstream scripts apply consistent strategy
```

---

## Key Insights from Analysis

### Current Input Data Profile

**Based on pipeline logs analysis**:
- ‚úì **187,234 faults** across **3,165 unique equipment**
- ‚úì **6.5 years** of data coverage (2019-2025)
- ‚úì **88.5%** of data before cutoff (2024-06-25)
- ‚úì **11.5%** of data after cutoff (for targets)
- ‚úì **24 columns** with mixed data types
- ‚úì **Turkish encoding** (UTF-8 compatible)

### Data Quality Issues Identified

**From previous pipeline runs**:
- ‚ö†Ô∏è ~20% missing cbs_id (acceptable, domain decision)
- ‚ö†Ô∏è Some features with >50% missing (need Phase 1.5 decision)
- ‚ö†Ô∏è Text encoding issues with Turkish characters (handled in scripts)
- ‚úì No corrupt date records
- ‚úì No obvious duplicate faults

---

## Recommendations

### For Your Current Analysis

1. **Run Script 00 First**:
   ```bash
   python 00_input_data_source_analysis.py
   ```
   This confirms data is ready before testing Phase 1 fixes

2. **Review Output Carefully**:
   - Note any missing columns
   - Check temporal coverage
   - Review recommendations
   - Validate quality assessment

3. **Address Any Critical Issues**:
   - Missing key columns ‚Üí Contact data source
   - Low temporal coverage ‚Üí Check date ranges
   - Encoding issues ‚Üí Verify file format
   - Low data volume ‚Üí Check filtering

4. **Document Findings**:
   - Save analysis output
   - Note data assumptions
   - Record any manual corrections
   - Create data dictionary

### For Future Pipeline Runs

1. **Always Start with Script 00**
   - Make it part of your standard workflow
   - Protects against data format changes
   - Documents each run's data state
   - Early warning system for issues

2. **Integrate into Automation**
   - Add to scheduled data validation
   - Alert on quality degradation
   - Track data evolution over time
   - Create audit trail

---

## Benefits of This Three-Layer Approach

| Layer | Purpose | Benefit |
|-------|---------|---------|
| **Input Analysis** | Pre-pipeline validation | Catches issues before wasted computation |
| **Data Profiling** | Quality assessment | Validates assumptions, documents strategy |
| **EDA** | Feature insights | Explains patterns, drives optimization |

---

## Conclusion

**Before testing Phase 1 fixes, you should**:

1. ‚úÖ Run `00_input_data_source_analysis.py`
2. ‚úÖ Review recommendations and validate data quality
3. ‚úÖ Confirm input file is ready
4. ‚úÖ Then proceed with full pipeline using Phase 1 fixes

This ensures:
- Input data is valid ‚úì
- Columns are correct ‚úì
- Encoding is proper ‚úì
- Temporal coverage adequate ‚úì
- Quality baseline established ‚úì

**Then you can safely test Phase 1 fixes with confidence!**

---

**Status**: Ready to validate input data and test Phase 1 fixes
**Next**: Run script 00, then full pipeline with Phase 1 corrections
