# üîç PIPELINE REVIEW SUMMARY - PoF2 Equipment Failure Prediction

**Date**: 2025-01-XX
**Session**: Pipeline Optimization & Validation
**Status**: ‚úÖ Critical Issues Fixed | ‚ö†Ô∏è Validation & Documentation Required

---

## üìä EXECUTIVE SUMMARY

Your pipeline has **no data leakage** in features (confirmed via audit), but has **two critical issues** that inflate performance metrics:

1. **‚ùå Random Train/Test Split** ‚Üí Temporal leakage (equipment from 2023 in train, 2021 in test)
2. **‚ùå Severe Sample Size Problem** ‚Üí 0.9-1.8 events/feature (need 10-20)

**Current Performance**: AUC 0.94-0.97 (suspiciously high)
**Expected After Fixes**: AUC 0.75-0.85 (realistic for temporal prediction)

---

## üêõ ISSUES FOUND & FIXED

### ‚úÖ **ISSUE 1: Target Creation Bug (FIXED)**
**File**: `06_temporal_pof_model.py`

**Problem**:
- 3M and 24M targets were **IDENTICAL** (both had 85 positives)
- Code only created `future_faults_6M` and `future_faults_12M`
- if/else defaulted all non-6M horizons to `future_faults_12M`

**Fix Applied** (Commit `8749578`):
- Created all 4 future prediction windows (3M, 6M, 12M, 24M)
- Properly map each horizon to its corresponding failure window

**Result**:
- ‚úÖ 3M now has 29 positives (was 85) - **FIXED**
- ‚úÖ 6M has 41 positives (unchanged) - **CORRECT**
- ‚úÖ 12M has 59 positives (unchanged) - **CORRECT**
- ‚ö†Ô∏è 24M has 59 positives (same as 12M) - **DATA LIMITATION** (see Issue 3)

---

### ‚úÖ **ISSUE 2: MTBF Feature Naming Mismatch (FIXED)**
**File**: `config.py`

**Problem**:
- `MTBF_G√ºn` (PRIMARY PoF predictor) was removed by VIF despite being critical
- `config.py` protected wrong name: `MTBF_InterFault_G√ºn`
- `02_data_transformation.py` creates: `MTBF_G√ºn`
- Result: Feature selection removed the primary MTBF feature at VIF=20

**Fix Applied** (Commit `ccc41c7`):
1. Renamed `MTBF_InterFault_G√ºn` ‚Üí `MTBF_G√ºn` (correct name)
2. Renamed `MTBF_ActiveLife_G√ºn` ‚Üí `MTBF_Observable_G√ºn` (Method 3)
3. Added `Baseline_Hazard_Rate` to protected (Cox model needs this)
4. Added `MTBF_Degradation_Ratio` to protected
5. Removed `Toplam_Arƒ±za_Sayisi_Lifetime` (confirmed leaky)

**Protected MTBF Features** (now 7 total):
- `MTBF_G√ºn` - Method 1: PRIMARY PoF predictor
- `MTBF_Lifetime_G√ºn` - Method 2: Survival analysis baseline
- `MTBF_Observable_G√ºn` - Method 3: Degradation detection
- `Baseline_Hazard_Rate` - 1/MTBF_Lifetime for Cox model
- `MTBF_Degradation_Ratio` - Failure acceleration detector
- `MTBF_InterFault_Trend` - Degradation trend proxy
- `MTBF_InterFault_StdDev` - Predictability variance

---

### ‚ö†Ô∏è **ISSUE 3: 24M = 12M (DATA LIMITATION)**

**Diagnosis**:
- Data ends at 2025-06-25 (12 months after cutoff)
- Only **3 additional equipment** fail between 12M-24M windows
- These 3 equipment are NOT in training set (in excluded 172 post-cutoff)
- Therefore, 12M and 24M targets are **IDENTICAL** for training data

**Evidence**:
```
All Equipment (734 total):
  12M window: 254 equipment fail
  24M window: 257 equipment fail (+3)

Training Set (562 filtered):
  12M: 59 positives
  24M: 59 positives (IDENTICAL - the +3 aren't in training set)
```

**‚úÖ RECOMMENDATION**: **Remove 24M horizon** from analysis (no predictive value over 12M)

**Action Required**:
```python
# In config.py, change:
HORIZONS = {'3M': 90, '6M': 180, '12M': 365, '24M': 730}
# To:
HORIZONS = {'3M': 90, '6M': 180, '12M': 365}
# Note: 24M removed - insufficient data differentiation
```

---

### ‚ùå **ISSUE 4: Random Split Temporal Leakage (CRITICAL)**

**Problem**:
- Current: Random 70/30 split mixes equipment from different years
- Equipment from 2023 in training, 2021 in test = **model sees the future!**
- This inflates AUC (0.94-0.97) above realistic levels

**Evidence**:
- Test AUC **HIGHER** than CV AUC (should be equal or lower):
  - 3M: CV=0.9036 ‚Üí Test=0.9733 (+7%!) ‚Üê Overfitting indicator
  - 6M: CV=0.9030 ‚Üí Test=0.9485 (+5%)

**‚úÖ SOLUTION**: Use **walk-forward validation** (time-based splits)

**Script Created**: `07_walkforward_validation.py`

**Approach**:
- Fold 1: Train on 2020-2021, test on 2022
- Fold 2: Train on 2020-2022, test on 2023
- Fold 3: Train on 2020-2023, test on 2024

**Expected Outcome**: AUC should **decrease** to 0.75-0.85 (realistic)

**‚úÖ Action Required**: Run the script and compare results:
```bash
python 07_walkforward_validation.py
```

---

### ‚ùå **ISSUE 5: Sample Size Problem (SEVERE)**

**Problem**: Not enough positive samples per feature

| Horizon | Positive Samples | Features | Events/Feature | Status |
|---------|-----------------|----------|----------------|---------|
| 3M | 29 | 32 | **0.9** | ‚ùå SEVERE RISK |
| 6M | 41 | 32 | **1.3** | ‚ùå SEVERE RISK |
| 12M | 59 | 32 | **1.8** | ‚ùå HIGH RISK |

**Rule of Thumb**: Need **10-20 events per feature** for stable models
**Your Reality**: **0.9-1.8 events/feature** = **10-20x FEWER than recommended**

**Why This Matters**:
- Model **memorizes training data** instead of learning patterns
- Coefficients are **unstable** (small data changes = big prediction changes)
- High AUC is due to **overfitting**, not genuine predictive power

**Solutions**:

**Option A**: **Accept with Documentation** (Recommended)
- ‚úÖ Document limitation clearly
- ‚úÖ Use for **relative ranking only** (high/medium/low risk)
- ‚ùå Don't treat probabilities as calibrated failure rates

**Option B**: **Reduce Features to 15-20**
- Keep only top 15 features by importance
- Improves ratio: 3M: 29/15 = 1.9 events/feature (better, but still low)
- Trade-off: Lower AUC, but more stable

**Option C**: **Collect More Data** (Long-term)
- Need 300+ positive samples per horizon
- Expand dataset from 562 to ~2,000 equipment
- This is the ONLY real solution

---

### ‚ö†Ô∏è **ISSUE 6: Class Imbalance (Equipment Type)**

**Problem**: Some equipment types have <20 samples ‚Üí Stratification fails

**‚úÖ SOLUTION**: Equipment Type Grouping

**Script Created**: `08_class_imbalance_analysis.py`

**What It Does**:
1. Identifies rare equipment classes (<20 samples)
2. Groups rare types into broader categories:
   - Switches/Disconnectors ‚Üí `Switch_Disconnector`
   - Circuit Breakers ‚Üí `Circuit_Breaker`
   - Transformers ‚Üí `Transformer`
   - Rare types ‚Üí `Other_Equipment`
3. Creates `Equipment_Group` column for stratified sampling
4. Analyzes failure rate by equipment type
5. Checks SMOTE feasibility

**‚úÖ Action Required**: Run the script:
```bash
python 08_class_imbalance_analysis.py
```

---

## üéØ SOLUTIONS PROVIDED

### 1Ô∏è‚É£ **Walk-Forward Validation** (PRIORITY 1)
**File**: `07_walkforward_validation.py`

**What It Does**:
- ‚úÖ Time-based train/test splits (prevents temporal leakage)
- ‚úÖ Tests model stability over time
- ‚úÖ Detects concept drift (failure patterns changing)
- ‚úÖ Temporal data quality checks:
  - Faults before installation date
  - Last fault after cutoff (data leakage check)
  - Age calculation consistency
  - Negative time intervals

**Expected Results**:
- Walk-forward AUC will be **LOWER** than random split (0.75-0.85)
- This is **MORE REALISTIC** - not inflated by temporal leakage
- If AUC variance is HIGH across folds ‚Üí concept drift detected

**Run It**:
```bash
python 07_walkforward_validation.py
```

---

### 2Ô∏è‚É£ **Class Imbalance Analysis**
**File**: `08_class_imbalance_analysis.py`

**What It Does**:
- ‚úÖ Target class distribution (positive vs negative)
- ‚úÖ Equipment type distribution (identifies rare classes)
- ‚úÖ Cross-tabulation: Equipment type √ó Failure rate
- ‚úÖ Equipment type grouping (merge rare classes)
- ‚úÖ Stratified sampling recommendations
- ‚úÖ SMOTE feasibility check per horizon

**Outputs**:
- `data/equipment_grouping.csv` - Equipment type mappings

**Run It**:
```bash
python 08_class_imbalance_analysis.py
```

---

### 3Ô∏è‚É£ **SMOTE Training (Optional)**
**File**: `09_train_with_smote.py`

**What It Does**:
- Creates **synthetic samples** for minority class
- Compares baseline vs SMOTE performance
- Recommends whether to use SMOTE based on AUC improvement

**‚ö†Ô∏è WARNING**: Use with caution!
- SMOTE creates **fake data** (not real failures)
- Can cause overfitting on synthetic patterns
- Best for 6M/12M (40+ samples)
- **NOT recommended for 3M** (only 29 samples)

**Run It** (Optional):
```bash
python 09_train_with_smote.py
```

---

## üìã ACTION PLAN

### ‚úÖ **Completed**
1. Fixed 3M target creation bug
2. Fixed MTBF feature naming mismatch
3. Created walk-forward validation script
4. Created class imbalance analysis script
5. Created SMOTE training script (optional)
6. Diagnosed 24M = 12M issue (data limitation)
7. Identified sample size problem (0.9-1.8 events/feature)

### üîß **Required Actions**

#### **PRIORITY 1: Validate Performance with Walk-Forward**
```bash
python 07_walkforward_validation.py
```
Expected: AUC drops to 0.75-0.85 (more realistic)

#### **PRIORITY 2: Analyze Class Imbalance**
```bash
python 08_class_imbalance_analysis.py
```
Output: Equipment grouping for stratified sampling

#### **PRIORITY 3: Remove 24M Horizon**
Edit `config.py`:
```python
# Change this:
HORIZONS = {'3M': 90, '6M': 180, '12M': 365, '24M': 730}

# To this:
HORIZONS = {'3M': 90, '6M': 180, '12M': 365}
```

#### **PRIORITY 4: Document Model Limitations**
Add to model documentation:

```markdown
‚ö†Ô∏è MODEL LIMITATIONS:

1. Sample Size Constraint:
   - 3M: 29 positives / 32 features = 0.9 events/feature (SEVERE overfitting risk)
   - 6M: 41 positives / 32 features = 1.3 events/feature (SEVERE overfitting risk)
   - 12M: 59 positives / 32 features = 1.8 events/feature (HIGH overfitting risk)
   - Recommended: 10-20 events/feature for stable models

2. Performance Interpretation:
   - AUC 0.94-0.97 is inflated due to overfitting on small sample
   - Test > CV AUC indicates model memorization
   - Use for RELATIVE RANKING only (high/medium/low risk)
   - Do NOT treat predicted probabilities as calibrated failure rates

3. Data Availability:
   - 24M horizon removed (only 3 additional failures beyond 12M)
   - Effective horizons: 3M, 6M, 12M only
   - Data ends at 2025-06-25 (12 months after cutoff)

4. Recommended Use:
   - Rank equipment by relative risk
   - Prioritize maintenance by predicted risk tier
   - Do NOT use probabilities for financial risk calculations
   - Re-train with more data when available (target: 300+ positives per horizon)
```

#### **OPTIONAL: Experiment with SMOTE**
```bash
python 09_train_with_smote.py
```
Only if you're comfortable with synthetic data

---

## üìä EXPECTED OUTCOMES

### **Before Fixes**:
- Random split AUC: 0.94-0.97 (inflated)
- 24M = 12M (identical metrics)
- Missing MTBF_G√ºn feature
- Test > CV AUC (overfitting)

### **After Fixes**:
- Walk-forward AUC: 0.75-0.85 (realistic) ‚úÖ
- 24M removed (no value over 12M) ‚úÖ
- MTBF features restored (7 total) ‚úÖ
- More stable cross-validation ‚úÖ
- Equipment grouping enables stratification ‚úÖ

---

## üéì KEY LEARNINGS

1. **Random split is WRONG for temporal prediction**
   - Always use time-based validation for temporal data
   - Equipment from 2023 in train, 2021 in test = future leakage

2. **High AUC ‚â† Good Model**
   - AUC 0.94-0.97 was due to overfitting, not genuine performance
   - With 0.9-1.8 events/feature, any complex model will overfit

3. **Sample size is the limiting factor**
   - 562 equipment with 29-59 positives is TOO SMALL
   - Need 10-20x more data for stable 32-feature models
   - No amount of feature engineering can fix insufficient data

4. **Protected features must match actual feature names**
   - `MTBF_InterFault_G√ºn` (wrong) vs `MTBF_G√ºn` (actual)
   - Always verify feature names in Step 2 output

5. **Data availability constrains prediction horizons**
   - Can't predict 24M if data only goes to 12M
   - Check data range before defining prediction windows

---

## üìÅ FILES CREATED

| File | Purpose | Priority |
|------|---------|----------|
| `07_walkforward_validation.py` | Time-based validation | ‚≠ê‚≠ê‚≠ê CRITICAL |
| `08_class_imbalance_analysis.py` | Equipment grouping | ‚≠ê‚≠ê HIGH |
| `09_train_with_smote.py` | SMOTE experiment | ‚≠ê OPTIONAL |
| `check_data_availability.py` | Data range diagnostics | ‚úÖ USED |
| `diagnostic_model_audit.py` | Performance audit | ‚úÖ USED |
| `PIPELINE_REVIEW_SUMMARY.md` | This document | üìñ REFERENCE |

---

## üöÄ NEXT STEPS

1. **Run walk-forward validation** (`07_walkforward_validation.py`)
2. **Run imbalance analysis** (`08_class_imbalance_analysis.py`)
3. **Remove 24M horizon** (config.py)
4. **Document limitations** (model README)
5. **Share walk-forward results** for final assessment
6. **(Optional)** Experiment with SMOTE (`09_train_with_smote.py`)

---

## ‚úÖ CONCLUSION

Your pipeline is **technically correct** (no data leakage in features), but has **methodological issues** (random split, small sample size) that inflate performance metrics.

**Key Takeaway**: The high AUC (0.94-0.97) is due to **overfitting on small sample + temporal leakage from random split**, NOT genuine predictive power.

After implementing walk-forward validation and documenting limitations, your models will have:
- ‚úÖ More realistic performance estimates (0.75-0.85 AUC)
- ‚úÖ Better understanding of model stability over time
- ‚úÖ Proper equipment type stratification
- ‚úÖ Clear documentation of limitations for stakeholders

**The models are still USEFUL for relative ranking** - just not as "perfect" as the inflated metrics suggest!

---

**Questions?** Review this document and run the diagnostic scripts. Share results for final assessment.
