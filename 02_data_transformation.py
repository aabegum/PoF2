"""
DATA TRANSFORMATION: FAULT-LEVEL â†’ EQUIPMENT-LEVEL v3.0 (ENHANCED)
Turkish EDAÅ PoF Prediction Project

ENHANCEMENTS in v3.0:
âœ“ Day-precision age calculation (not just year)
âœ“ Improved date validation with diagnostics
âœ“ Optional first work order fallback for missing ages
âœ“ Vectorized operations for better performance
âœ“ Complete audit trail (install date, age source, age in days)

Key Features:
âœ“ Smart Equipment ID (cbs_id â†’ Ekipman ID â†’ HEPSI_ID â†’ Ekipman Kodu)
âœ“ Unified Equipment Classification (Equipment_Type â†’ Ekipman SÄ±nÄ±fÄ± â†’ fallbacks)
âœ“ Age source tracking (TESIS_TARIHI vs EDBS_IDATE vs FIRST_WORKORDER_PROXY)
âœ“ Handles invalid dates (1900-01-01, 00:00:00, nulls)
âœ“ Failure history aggregation (3/6/12 months)
âœ“ MTBF calculation
âœ“ Recurring fault detection (30/90 days)
âœ“ Customer impact columns (all MV/LV categories)
âœ“ Optional specifications (voltage_level, kVa_rating) - future-proof

Priority Logic:
- Equipment ID: cbs_id â†’ Ekipman ID â†’ HEPSI_ID â†’ Ekipman Kodu
- Equipment Class: Equipment_Type â†’ Ekipman SÄ±nÄ±fÄ± â†’ Kesinti Ekipman SÄ±nÄ±fÄ±
- Installation Date: TESIS_TARIHI â†’ EDBS_IDATE â†’ First Work Order (optional)

Input:  data/combined_data.xlsx (fault records)
Output: data/equipment_level_data.csv (equipment records with ~30+ features)
"""

import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime, timedelta
import warnings
import sys
warnings.filterwarnings('ignore')

# Fix Unicode encoding for Windows console (Turkish cp1254 issue)
if sys.platform == 'win32':
    try:
        import ctypes
        ctypes.windll.kernel32.SetConsoleCP(65001)
        ctypes.windll.kernel32.SetConsoleOutputCP(65001)
        sys.stdout.reconfigure(encoding='utf-8')
    except Exception:
        pass

pd.set_option('display.max_columns', None)

# ============================================================================
# CONFIGURATION
# ============================================================================

# Constants
CURRENT_YEAR = 2025
MIN_VALID_YEAR = 1950
MAX_VALID_YEAR = 2025
REFERENCE_DATE = pd.Timestamp('2025-06-25')

# Feature flags
USE_FIRST_WORKORDER_FALLBACK = True  # Set to True to enable Option 3 (first work order as age proxy)

print("="*100)
print(" "*25 + "DATA TRANSFORMATION PIPELINE v3.0 (ENHANCED)")
print("="*100)
print(f"\nâš™ï¸  Configuration:")
print(f"   Reference Date: {REFERENCE_DATE.strftime('%Y-%m-%d')}")
print(f"   Valid Year Range: {MIN_VALID_YEAR}-{MAX_VALID_YEAR}")
print(f"   First Work Order Fallback: {'ENABLED' if USE_FIRST_WORKORDER_FALLBACK else 'DISABLED'}")

# ============================================================================
# STEP 1: LOAD DATA
# ============================================================================
print("\n" + "="*100)
print("STEP 1: LOADING FAULT-LEVEL DATA")
print("="*100)

df = pd.read_excel('data/combined_data.xlsx')
print(f"\nâœ“ Loaded: {df.shape[0]:,} faults Ã— {df.shape[1]} columns")
original_fault_count = len(df)

# ============================================================================
# STEP 2: ENHANCED DATE PARSING & VALIDATION
# ============================================================================
print("\n" + "="*100)
print("STEP 2: PARSING AND VALIDATING DATE COLUMNS (ENHANCED)")
print("="*100)

def parse_and_validate_date(date_series, column_name, min_year=MIN_VALID_YEAR, max_year=MAX_VALID_YEAR, report=True):
    """
    Parse and validate dates with detailed diagnostics

    Args:
        date_series: Series of date values
        column_name: Name for reporting
        min_year: Minimum valid year (default: 1950)
        max_year: Maximum valid year (default: 2025)
        report: Whether to print statistics (default: True)

    Returns:
        Series of validated datetime values (invalid â†’ NaT)
    """
    # Check if data is Excel serial date (integer/float format)
    if pd.api.types.is_numeric_dtype(date_series):
        # Excel serial dates: days since 1900-01-01 (Windows Excel)
        # Valid range: ~18263 (1950) to ~45657 (2025)
        # Origin = 1899-12-30 because Excel incorrectly treats 1900 as leap year
        parsed = pd.to_datetime(date_series, unit='D', origin='1899-12-30', errors='coerce')
    else:
        # Parse dates with Turkish date format support (DD/MM/YYYY)
        parsed = pd.to_datetime(date_series, errors='coerce', dayfirst=True)

    # Validation masks
    valid_mask = (
        parsed.notna() &
        (parsed.dt.year >= min_year) &
        (parsed.dt.year <= max_year)
    )

    # Categorize invalid dates
    invalid_old = (parsed.notna() & (parsed.dt.year < min_year)).sum()
    invalid_future = (parsed.notna() & (parsed.dt.year > max_year)).sum()

    # Set invalid to NaT
    parsed[~valid_mask] = pd.NaT

    # Statistics
    if report:
        total = len(date_series)
        valid = valid_mask.sum()

        print(f"\n  {column_name:30s}:")
        print(f"    Valid dates:       {valid:6,}/{total:6,} ({valid/total*100:5.1f}%)")
        if invalid_old > 0:
            print(f"    Invalid (< {min_year}): {invalid_old:6,} âš ï¸  (set to NaT)")
        if invalid_future > 0:
            print(f"    Invalid (> {max_year}): {invalid_future:6,} âš ï¸  (set to NaT)")

    return parsed

# Parse and validate all date columns
print("\nParsing installation date columns:")
df['TESIS_TARIHI_parsed'] = parse_and_validate_date(df['TESIS_TARIHI'], 'TESIS_TARIHI')
df['EDBS_IDATE_parsed'] = parse_and_validate_date(df['EDBS_IDATE'], 'EDBS_IDATE')

print("\nParsing fault timestamp columns:")
df['started at'] = parse_and_validate_date(df['started at'], 'started at', min_year=2020, report=True)
df['ended at'] = parse_and_validate_date(df['ended at'], 'ended at', min_year=2020, report=True)

# Parse work order creation date (for fallback option)
if 'OluÅŸturma Tarihi SÄ±ralama' in df.columns or 'OluÅŸturulma_Tarihi' in df.columns:
    creation_col = 'OluÅŸturma Tarihi SÄ±ralama' if 'OluÅŸturma Tarihi SÄ±ralama' in df.columns else 'OluÅŸturulma_Tarihi'
    df['OluÅŸturulma_Tarihi'] = parse_and_validate_date(df[creation_col], 'Work Order Creation Date', min_year=2015, report=True)
else:
    df['OluÅŸturulma_Tarihi'] = pd.NaT
    print("\n  âš ï¸  Work order creation date not found (fallback option disabled)")

# ============================================================================
# STEP 3: ENHANCED EQUIPMENT AGE CALCULATION
# ============================================================================
print("\n" + "="*100)
print("STEP 3: CALCULATING EQUIPMENT AGE (DAY PRECISION)")
print("="*100)

def calculate_equipment_age_improved(row):
    """
    Calculate equipment age with day precision

    Priority:
    1. TESIS_TARIHI (primary installation date)
    2. EDBS_IDATE (fallback installation date)
    3. First work order date (optional proxy - equipment may be older)

    Returns:
        tuple: (age_in_days, source_used, install_date)
    """
    ref_date = REFERENCE_DATE

    # Option 1: TESIS_TARIHI (primary)
    if pd.notna(row['TESIS_TARIHI_parsed']):
        install_date = row['TESIS_TARIHI_parsed']
        if install_date < ref_date:
            age_days = (ref_date - install_date).days
            return age_days, 'TESIS_TARIHI', install_date

    # Option 2: EDBS_IDATE (fallback)
    if pd.notna(row['EDBS_IDATE_parsed']):
        install_date = row['EDBS_IDATE_parsed']
        if install_date < ref_date:
            age_days = (ref_date - install_date).days
            return age_days, 'EDBS_IDATE', install_date

    # No valid installation date found
    return None, 'MISSING', None

print("\nCalculating ages from installation dates...")

# Optimized tuple unpacking (vectorized)
results = df.apply(calculate_equipment_age_improved, axis=1, result_type='expand')
results.columns = ['Ekipman_YaÅŸÄ±_GÃ¼n', 'YaÅŸ_Kaynak', 'Ekipman_Kurulum_Tarihi']

# Assign all at once
df[['Ekipman_YaÅŸÄ±_GÃ¼n', 'YaÅŸ_Kaynak', 'Ekipman_Kurulum_Tarihi']] = results
df['Ekipman_YaÅŸÄ±_YÄ±l'] = df['Ekipman_YaÅŸÄ±_GÃ¼n'] / 365.25

# Statistics
print("\nâœ“ Age Calculation Results:")
print(f"\n  Age Source Distribution:")
source_counts = df['YaÅŸ_Kaynak'].value_counts()
for source, count in source_counts.items():
    pct = count / len(df) * 100
    print(f"    {source:25s}: {count:6,} ({pct:5.1f}%)")

# Age statistics (excluding missing)
valid_ages = df[df['YaÅŸ_Kaynak'] != 'MISSING']['Ekipman_YaÅŸÄ±_YÄ±l']
if len(valid_ages) > 0:
    print(f"\n  Age Statistics (valid ages only):")
    print(f"    Mean:   {valid_ages.mean():>6.1f} years")
    print(f"    Median: {valid_ages.median():>6.1f} years")
    print(f"    Min:    {valid_ages.min():>6.1f} years")
    print(f"    Max:    {valid_ages.max():>6.1f} years")

    # Age distribution
    age_bins = [0, 5, 10, 20, 30, 50, 75]
    age_labels = ['0-5 yrs', '5-10 yrs', '10-20 yrs', '20-30 yrs', '30-50 yrs', '50-75 yrs']
    age_dist = pd.cut(valid_ages, bins=age_bins, labels=age_labels).value_counts().sort_index()

    print(f"\n  Age Distribution:")
    for label, count in age_dist.items():
        pct = count / len(valid_ages) * 100
        bar = 'â–ˆ' * int(pct / 2)  # Visual bar
        print(f"    {label}: {count:>4,} ({pct:>5.1f}%) {bar}")

    # Warnings
    if (valid_ages > 75).sum() > 0:
        print(f"\n  âš ï¸  WARNING: {(valid_ages > 75).sum()} equipment > 75 years (check data quality!)")
    if valid_ages.median() < 1:
        print(f"  âš ï¸  WARNING: Median age is {valid_ages.median():.1f} years - investigate if accurate")

# ============================================================================
# STEP 3B: OPTIONAL FIRST WORK ORDER FALLBACK
# ============================================================================
if USE_FIRST_WORKORDER_FALLBACK:
    print("\n" + "="*100)
    print("STEP 3B: FILLING MISSING AGES WITH FIRST WORK ORDER (VECTORIZED)")
    print("="*100)

    missing_mask = df['YaÅŸ_Kaynak'] == 'MISSING'
    missing_count = missing_mask.sum()

    if missing_count > 0 and 'OluÅŸturulma_Tarihi' in df.columns:
        print(f"\n  Equipment with MISSING age: {missing_count:,} ({missing_count/len(df)*100:.1f}%)")
        print(f"  Attempting to use first work order date as proxy...\n")

        # Identify equipment ID column
        equip_id_cols = ['cbs_id', 'Ekipman Kodu', 'Ekipman ID', 'HEPSI_ID']
        equip_id_col = None
        for col in equip_id_cols:
            if col in df.columns:
                equip_id_col = col
                break

        if equip_id_col:
            print(f"  Using equipment ID column: {equip_id_col}")

            # Vectorized approach: Get first work order per equipment
            first_wo_dates = df.groupby(equip_id_col)['OluÅŸturulma_Tarihi'].min()

            # Map first work order dates to all rows
            df['_first_wo'] = df[equip_id_col].map(first_wo_dates)

            # Calculate age from first work order (vectorized)
            age_from_wo = (REFERENCE_DATE - df['_first_wo']).dt.days

            # Only fill where: missing AND first_wo is valid AND age is positive
            fill_mask = (
                missing_mask &
                df['_first_wo'].notna() &
                (age_from_wo > 0)
            )

            # Vectorized assignment
            df.loc[fill_mask, 'Ekipman_YaÅŸÄ±_GÃ¼n'] = age_from_wo[fill_mask]
            df.loc[fill_mask, 'Ekipman_YaÅŸÄ±_YÄ±l'] = age_from_wo[fill_mask] / 365.25
            df.loc[fill_mask, 'YaÅŸ_Kaynak'] = 'FIRST_WORKORDER_PROXY'
            df.loc[fill_mask, 'Ekipman_Kurulum_Tarihi'] = df.loc[fill_mask, '_first_wo']

            # Cleanup temporary column
            df.drop(columns=['_first_wo'], inplace=True)

            filled_count = fill_mask.sum()
            remaining_missing = (df['YaÅŸ_Kaynak'] == 'MISSING').sum()

            print(f"  âœ“ Filled: {filled_count:,} using first work order proxy")
            print(f"  âœ“ Remaining MISSING: {remaining_missing:,} ({remaining_missing/len(df)*100:.1f}%)")

            # Final age statistics
            if filled_count > 0:
                print(f"\n  Updated Age Source Distribution:")
                for source, count in df['YaÅŸ_Kaynak'].value_counts().items():
                    pct = count / len(df) * 100
                    print(f"    {source:25s}: {count:6,} ({pct:5.1f}%)")
        else:
            print(f"  âš ï¸  Equipment ID column not found - cannot use first work order fallback")
    elif missing_count == 0:
        print(f"\n  âœ“ No missing ages - first work order fallback not needed")
    else:
        print(f"\n  âš ï¸  Work order creation date not available - cannot use fallback")

# ============================================================================
# STEP 4: PROCESS FAULT TIMESTAMPS
# ============================================================================
print("\n" + "="*100)
print("STEP 4: PROCESSING FAULT TIMESTAMPS")
print("="*100)

df['Fault_Month'] = df['started at'].dt.month
df['Summer_Peak_Flag'] = df['Fault_Month'].isin([6, 7, 8, 9]).astype(int)
df['Winter_Peak_Flag'] = df['Fault_Month'].isin([12, 1, 2]).astype(int)
df['Time_To_Repair_Hours'] = (df['ended at'] - df['started at']).dt.total_seconds() / 3600

print("\nâœ“ Temporal features created:")
print(f"  Summer peak faults: {df['Summer_Peak_Flag'].sum():,}")
print(f"  Winter peak faults: {df['Winter_Peak_Flag'].sum():,}")
print(f"  Avg repair time: {df['Time_To_Repair_Hours'].mean():.1f} hours")

# ============================================================================
# STEP 5: CALCULATE FAILURE PERIODS
# ============================================================================
print("\n" + "="*100)
print("STEP 5: CALCULATING FAILURE PERIOD FLAGS")
print("="*100)

reference_date = df['started at'].max()
cutoff_3m = reference_date - pd.Timedelta(days=90)
cutoff_6m = reference_date - pd.Timedelta(days=180)
cutoff_12m = reference_date - pd.Timedelta(days=365)

df['Fault_Last_3M'] = (df['started at'] >= cutoff_3m).astype(int)
df['Fault_Last_6M'] = (df['started at'] >= cutoff_6m).astype(int)
df['Fault_Last_12M'] = (df['started at'] >= cutoff_12m).astype(int)

print(f"\nâœ“ Failure period flags created:")
print(f"  Reference date: {reference_date.strftime('%Y-%m-%d')}")
print(f"  Faults in last 3M:  {df['Fault_Last_3M'].sum():,}")
print(f"  Faults in last 6M:  {df['Fault_Last_6M'].sum():,}")
print(f"  Faults in last 12M: {df['Fault_Last_12M'].sum():,}")

# ============================================================================
# STEP 6: IDENTIFY PRIMARY EQUIPMENT ID
# ============================================================================
print("\n" + "="*100)
print("STEP 6: EQUIPMENT IDENTIFICATION")
print("="*100)

# PRIMARY STRATEGY: cbs_id â†’ Ekipman ID â†’ HEPSI_ID â†’ Ekipman Kodu
print("\n--- Smart Equipment ID Selection ---")

# Create unified equipment ID with fallback logic
def get_equipment_id(row):
    """
    Get equipment ID with smart fallback
    Priority: cbs_id â†’ Ekipman ID â†’ HEPSI_ID â†’ Ekipman Kodu
    """
    if pd.notna(row.get('cbs_id')):
        return row['cbs_id']
    elif pd.notna(row.get('Ekipman ID')):
        return row['Ekipman ID']
    elif pd.notna(row.get('HEPSI_ID')):
        return row['HEPSI_ID']
    elif pd.notna(row.get('Ekipman Kodu')):
        return row['Ekipman Kodu']
    return None

df['Equipment_ID_Primary'] = df.apply(get_equipment_id, axis=1)

# Statistics
primary_coverage = df['Equipment_ID_Primary'].notna().sum()
unique_equipment = df['Equipment_ID_Primary'].nunique()

print(f"âœ“ Primary Equipment ID Strategy:")
print(f"  Priority 1: cbs_id")
print(f"  Priority 2: Ekipman ID")
print(f"  Priority 3: HEPSI_ID")
print(f"  Priority 4: Ekipman Kodu")
print(f"  Combined coverage: {primary_coverage:,} ({primary_coverage/len(df)*100:.1f}%)")
print(f"  Unique equipment: {unique_equipment:,}")
print(f"  Average faults per equipment: {len(df)/unique_equipment:.1f}")

# Use this as grouping key
equipment_id_col = 'Equipment_ID_Primary'

# ============================================================================
# STEP 6B: CREATE UNIFIED EQUIPMENT CLASSIFICATION
# ============================================================================
print("\n--- Smart Equipment Classification Selection ---")

# Create unified equipment class with fallback logic
def get_equipment_class(row):
    """
    Get equipment class with smart fallback
    Priority: Equipment_Type â†’ Ekipman SÄ±nÄ±fÄ± â†’ Kesinti Ekipman SÄ±nÄ±fÄ± â†’ Ekipman SÄ±nÄ±f
    """
    if pd.notna(row.get('Equipment_Type')):
        return row['Equipment_Type']
    elif pd.notna(row.get('Ekipman SÄ±nÄ±fÄ±')):
        return row['Ekipman SÄ±nÄ±fÄ±']
    elif pd.notna(row.get('Kesinti Ekipman SÄ±nÄ±fÄ±')):
        return row['Kesinti Ekipman SÄ±nÄ±fÄ±']
    elif pd.notna(row.get('Ekipman SÄ±nÄ±f')):
        return row['Ekipman SÄ±nÄ±f']
    return None

df['Equipment_Class_Primary'] = df.apply(get_equipment_class, axis=1)

class_coverage = df['Equipment_Class_Primary'].notna().sum()
print(f"âœ“ Unified Equipment Class created:")
print(f"  Priority: Equipment_Type â†’ Ekipman SÄ±nÄ±fÄ± â†’ Kesinti Ekipman SÄ±nÄ±fÄ±")
print(f"  Coverage: {class_coverage:,} ({class_coverage/len(df)*100:.1f}%)")
print(f"  Unique types (before harmonization): {df['Equipment_Class_Primary'].nunique()}")

# HARMONIZE EQUIPMENT CLASSES (fix synonyms and case sensitivity)
print("\n--- Equipment Class Harmonization ---")
equipment_class_mapping = {
    # Low Voltage Lines
    'aghat': 'AG Hat',
    'AG Hat': 'AG Hat',

    # Reclosers (case sensitivity)
    'REKORTMAN': 'Rekortman',
    'Rekortman': 'Rekortman',

    # Low Voltage Poles
    'agdirek': 'AG Direk',
    'AG Direk': 'AG Direk',

    # Transformers (consolidate variants)
    'OGAGTRF': 'OG/AG Trafo',
    'OG/AG Trafo': 'OG/AG Trafo',
    'Trafo Bina Tip': 'OG/AG Trafo',

    # Distribution Boxes/Panels
    'SDK': 'AG Pano Box',
    'AG Pano': 'AG Pano Box',

    # Disconnectors (standardize)
    'AyÄ±rÄ±cÄ±': 'AyÄ±rÄ±cÄ±',

    # Switches (standardize)
    'anahtar': 'AG Anahtar',
    'AG Anahtar': 'AG Anahtar',

    # Circuit Breakers (case sensitivity)
    'KESÄ°CÄ°': 'Kesici',
    'Kesici': 'Kesici',

    # Medium Voltage Lines
    'OGHAT': 'OG Hat',

    # Panels
    'PANO': 'Pano',

    # Buildings
    'Bina': 'Bina',

    # Lighting
    'ArmatÃ¼r': 'ArmatÃ¼r',

    # High Voltage Pole
    'ENHDirek': 'ENH Direk',
}

# Apply mapping
df['Equipment_Class_Primary'] = df['Equipment_Class_Primary'].map(
    lambda x: equipment_class_mapping.get(x, x) if pd.notna(x) else x
)

harmonized_classes = df['Equipment_Class_Primary'].nunique()
print(f"âœ“ Equipment classes harmonized:")
print(f"  Before: {len(equipment_class_mapping)} types â†’ After: {harmonized_classes} types")
print(f"\n  Consolidated mappings:")
print(f"    â€¢ aghat + AG Hat â†’ AG Hat")
print(f"    â€¢ REKORTMAN + Rekortman â†’ Rekortman")
print(f"    â€¢ agdirek + AG Direk â†’ AG Direk")
print(f"    â€¢ OGAGTRF + OG/AG Trafo + Trafo Bina Tip â†’ OG/AG Trafo")
print(f"    â€¢ SDK + AG Pano â†’ AG Pano Box")
print(f"    â€¢ anahtar + AG Anahtar â†’ AG Anahtar")

# Track age source
def get_age_source(row):
    """Track which column provided installation date"""
    return row['YaÅŸ_Kaynak']  # Already set in step 3

df['Age_Source'] = df['YaÅŸ_Kaynak']

# ============================================================================
# STEP 7: AGGREGATE TO EQUIPMENT LEVEL
# ============================================================================
print("\n" + "="*100)
print("STEP 7: AGGREGATING TO EQUIPMENT LEVEL")
print("="*100)

# Sort by Age_Source to prioritize TESIS_TARIHI when aggregating with 'first'
# This ensures that for equipment with multiple faults, we prefer TESIS_TARIHI over EDBS_IDATE
source_priority = {'TESIS_TARIHI': 0, 'EDBS_IDATE': 1, 'FIRST_WORKORDER_PROXY': 2, 'MISSING': 3}
df['_source_priority'] = df['Age_Source'].map(source_priority).fillna(99)
df = df.sort_values('_source_priority')
df = df.drop(columns=['_source_priority'])

print("\n  âœ“ Sorted data to prioritize TESIS_TARIHI as age source during aggregation")

# Build aggregation dictionary dynamically based on available columns
agg_dict = {
    # Equipment identification & classification
    'Equipment_Class_Primary': 'first',
    'Ekipman SÄ±nÄ±fÄ±': 'first',
    'Equipment_Type': 'first',
    'Kesinti Ekipman SÄ±nÄ±fÄ±': 'first',

    # Geographic data
    'KOORDINAT_X': 'first',
    'KOORDINAT_Y': 'first',
    'Ä°l': 'first',
    'Ä°lÃ§e': 'first',
    'Mahalle': 'first',

    # Age data (ENHANCED - TESIS_TARIHI prioritized via pre-sort)
    'Ekipman_Kurulum_Tarihi': 'first',
    'Ekipman_YaÅŸÄ±_GÃ¼n': 'first',
    'Ekipman_YaÅŸÄ±_YÄ±l': 'first',
    'Age_Source': 'first',

    # Fault history
    'started at': ['count', 'min', 'max'],
    'Fault_Last_3M': 'sum',
    'Fault_Last_6M': 'sum',
    'Fault_Last_12M': 'sum',

    # Temporal features
    'Summer_Peak_Flag': 'sum',
    'Winter_Peak_Flag': 'sum',
    'Time_To_Repair_Hours': ['mean', 'max']
}

# Add cause code column if available
if 'cause code' in df.columns:
    agg_dict['cause code'] = ['first', 'last', lambda x: x.mode()[0] if len(x.mode()) > 0 else None]
    print("\n  âœ“ Found: cause code (will aggregate first, last, and most common)")

# Add customer impact columns if available
customer_impact_cols = [
    'urban mv+suburban mv',
    'urban lv+suburban lv',
    'urban mv',
    'urban lv',
    'suburban mv',
    'suburban lv',
    'rural mv',
    'rural lv',
    'total customer count'
]

print("  Checking for customer impact columns...")
for col in customer_impact_cols:
    if col in df.columns:
        agg_dict[col] = ['mean', 'max']
        print(f"  âœ“ Found: {col}")

# Add optional specification columns if available
optional_spec_cols = {
    'voltage_level': 'first',
    'kVa_rating': 'first',
    'component voltage': 'first',
    'MARKA': 'first',
    'MARKA_MODEL': 'first',
    'FIRMA': 'first'
}

print("  Checking for optional specification columns...")
for col, agg_func in optional_spec_cols.items():
    if col in df.columns:
        agg_dict[col] = agg_func
        print(f"  âœ“ Found: {col}")

print(f"\nâœ“ Aggregating {len(df):,} fault records to equipment level...")
equipment_df = df.groupby(equipment_id_col).agg(agg_dict).reset_index()
equipment_df.columns = ['_'.join(col).strip('_') if col[1] else col[0] for col in equipment_df.columns.values]

print(f"âœ“ Created {len(equipment_df):,} equipment records from {original_fault_count:,} faults")

# ============================================================================
# STEP 8: RENAME COLUMNS
# ============================================================================
print("\n" + "="*100)
print("STEP 8: CREATING FINAL FEATURES")
print("="*100)

# Base rename dictionary (ENHANCED - includes new age columns)
rename_dict = {
    'Equipment_ID_Primary': 'Ekipman_ID',
    'Equipment_Class_Primary_first': 'Equipment_Class_Primary',
    'Ekipman SÄ±nÄ±fÄ±_first': 'Ekipman_SÄ±nÄ±fÄ±',
    'Equipment_Type_first': 'Equipment_Type',
    'Kesinti Ekipman SÄ±nÄ±fÄ±_first': 'Kesinti Ekipman SÄ±nÄ±fÄ±',
    'KOORDINAT_X_first': 'KOORDINAT_X',
    'KOORDINAT_Y_first': 'KOORDINAT_Y',
    'Ä°l_first': 'Ä°l',
    'Ä°lÃ§e_first': 'Ä°lÃ§e',
    'Mahalle_first': 'Mahalle',
    'Ekipman_Kurulum_Tarihi_first': 'Ekipman_Kurulum_Tarihi',  # NEW
    'Ekipman_YaÅŸÄ±_GÃ¼n_first': 'Ekipman_YaÅŸÄ±_GÃ¼n',  # NEW
    'Ekipman_YaÅŸÄ±_YÄ±l_first': 'Ekipman_YaÅŸÄ±_YÄ±l',
    'Age_Source_first': 'Age_Source',
    'started at_count': 'Toplam_ArÄ±za_Sayisi_Lifetime',
    'started at_min': 'Ä°lk_ArÄ±za_Tarihi',
    'started at_max': 'Son_ArÄ±za_Tarihi',
    'Fault_Last_3M_sum': 'ArÄ±za_SayÄ±sÄ±_3ay',
    'Fault_Last_6M_sum': 'ArÄ±za_SayÄ±sÄ±_6ay',
    'Fault_Last_12M_sum': 'ArÄ±za_SayÄ±sÄ±_12ay',
}

# Add cause code columns if available
if 'cause code_first' in equipment_df.columns:
    rename_dict['cause code_first'] = 'ArÄ±za_Nedeni_Ä°lk'
    rename_dict['cause code_last'] = 'ArÄ±za_Nedeni_Son'
    rename_dict['cause code_<lambda>'] = 'ArÄ±za_Nedeni_SÄ±k'

# Add customer impact columns dynamically
for col in customer_impact_cols:
    if f'{col}_mean' in equipment_df.columns:
        rename_dict[f'{col}_mean'] = f'{col.replace(" ", "_")}_Avg'
    if f'{col}_max' in equipment_df.columns:
        rename_dict[f'{col}_max'] = f'{col.replace(" ", "_")}_Max'

# Add optional specification columns dynamically
for col in optional_spec_cols.keys():
    if f'{col}_first' in equipment_df.columns:
        clean_col_name = col.replace(' ', '_')
        rename_dict[f'{col}_first'] = clean_col_name

equipment_df.rename(columns=rename_dict, inplace=True)

# ============================================================================
# STEP 9: CALCULATE CAUSE CODE FEATURES
# ============================================================================
has_cause_code = any(col for col in equipment_df.columns if 'cause code' in col.lower() or 'arÄ±za_nedeni' in col.lower())

if has_cause_code and 'cause code' in df.columns:
    print("\nCalculating cause code features...")

    # Create cause code distribution per equipment
    cause_distribution = df.groupby([equipment_id_col, 'cause code']).size().unstack(fill_value=0)

    # Cause diversity: How many different cause types per equipment
    equipment_df['ArÄ±za_Nedeni_Ã‡eÅŸitlilik'] = (cause_distribution > 0).sum(axis=1).reindex(equipment_df['Ekipman_ID']).fillna(0).values

    # Cause consistency: Percentage of faults with most common cause
    total_faults_per_equip = cause_distribution.sum(axis=1)
    max_cause_per_equip = cause_distribution.max(axis=1)
    cause_consistency = (max_cause_per_equip / total_faults_per_equip).reindex(equipment_df['Ekipman_ID']).fillna(0).values
    equipment_df['ArÄ±za_Nedeni_TutarlÄ±lÄ±k'] = cause_consistency

    print(f"  âœ“ Created ArÄ±za_Nedeni_Ã‡eÅŸitlilik (cause diversity)")
    print(f"  âœ“ Created ArÄ±za_Nedeni_TutarlÄ±lÄ±k (cause consistency)")
    print(f"  âœ“ Avg cause types per equipment: {equipment_df['ArÄ±za_Nedeni_Ã‡eÅŸitlilik'].mean():.2f}")
    print(f"  âœ“ Avg cause consistency: {equipment_df['ArÄ±za_Nedeni_TutarlÄ±lÄ±k'].mean():.2%}")
else:
    print("\nâš  Cause code column not found in fault data - skipping cause diversity/consistency features")

# ============================================================================
# STEP 10: CALCULATE MTBF
# ============================================================================
print("\nCalculating MTBF (Mean Time Between Failures)...")

def calculate_mtbf(row):
    if pd.notna(row['Ä°lk_ArÄ±za_Tarihi']) and pd.notna(row['Son_ArÄ±za_Tarihi']):
        total_days = (row['Son_ArÄ±za_Tarihi'] - row['Ä°lk_ArÄ±za_Tarihi']).days
        total_faults = row['Toplam_ArÄ±za_Sayisi_Lifetime']
        if total_faults > 1 and total_days > 0:
            return total_days / (total_faults - 1)
    return None

equipment_df['MTBF_GÃ¼n'] = equipment_df.apply(calculate_mtbf, axis=1)

# Days since last fault
equipment_df['Son_ArÄ±za_Gun_Sayisi'] = (REFERENCE_DATE - equipment_df['Son_ArÄ±za_Tarihi']).dt.days

print(f"  âœ“ MTBF calculable for {equipment_df['MTBF_GÃ¼n'].notna().sum():,} equipment")

# ============================================================================
# STEP 11: DETECT RECURRING FAULTS
# ============================================================================
print("\n" + "="*100)
print("STEP 11: DETECTING RECURRING FAULTS")
print("="*100)

def calculate_recurrence(equipment_id):
    equip_faults = df[df[equipment_id_col] == equipment_id]['started at'].dropna().sort_values()
    if len(equip_faults) < 2:
        return 0, 0
    time_diffs = equip_faults.diff().dt.days.dropna()
    return int((time_diffs <= 30).any()), int((time_diffs <= 90).any())

print("\nAnalyzing recurring fault patterns...")
recurrence_results = equipment_df['Ekipman_ID'].apply(calculate_recurrence)
equipment_df['Tekrarlayan_ArÄ±za_30gÃ¼n_Flag'] = [r[0] for r in recurrence_results]
equipment_df['Tekrarlayan_ArÄ±za_90gÃ¼n_Flag'] = [r[1] for r in recurrence_results]

print(f"âœ“ Recurring faults (30 days): {equipment_df['Tekrarlayan_ArÄ±za_30gÃ¼n_Flag'].sum():,} equipment")
print(f"âœ“ Recurring faults (90 days): {equipment_df['Tekrarlayan_ArÄ±za_90gÃ¼n_Flag'].sum():,} equipment")

# ============================================================================
# STEP 12: SAVE RESULTS
# ============================================================================
print("\n" + "="*100)
print("STEP 12: SAVING RESULTS")
print("="*100)

equipment_df.to_csv('data/equipment_level_data.csv', index=False, encoding='utf-8-sig')
print(f"\nâœ“ Saved: data/equipment_level_data.csv ({len(equipment_df):,} records)")

# Feature documentation
feature_docs = pd.DataFrame({
    'Feature_Name': equipment_df.columns,
    'Data_Type': equipment_df.dtypes.astype(str),
    'Completeness_%': (equipment_df.notna().sum() / len(equipment_df) * 100).round(1)
})
feature_docs.to_csv('data/feature_documentation.csv', index=False)
print(f"âœ“ Saved: data/feature_documentation.csv ({len(equipment_df.columns)} features)")

# ============================================================================
# FINAL SUMMARY
# ============================================================================
print("\n" + "="*100)
print("TRANSFORMATION COMPLETE!")
print("="*100)

print(f"\nğŸ“Š TRANSFORMATION SUMMARY:")
print(f"   â€¢ Input: {original_fault_count:,} fault records")
print(f"   â€¢ Output: {len(equipment_df):,} equipment records")
print(f"   â€¢ Reduction: {original_fault_count/len(equipment_df):.1f}x (faults per equipment)")
print(f"   â€¢ Total Features: {len(equipment_df.columns)} columns")

print(f"\nğŸ¯ KEY FEATURES CREATED:")
print(f"   â€¢ Equipment ID Strategy: cbs_id â†’ Ekipman ID â†’ HEPSI_ID â†’ Ekipman Kodu")
print(f"   â€¢ Equipment Classification: Equipment_Class_Primary (unified)")
print(f"   â€¢ Age Precision: DAY-LEVEL (not just year) âœ¨")
print(f"   â€¢ Age Sources: {equipment_df['Age_Source'].value_counts().to_dict()}")
print(f"   â€¢ Failure History: 3M, 6M, 12M fault counts")
print(f"   â€¢ MTBF: {equipment_df['MTBF_GÃ¼n'].notna().sum():,} equipment with valid MTBF")
print(f"   â€¢ Recurring Faults: {equipment_df['Tekrarlayan_ArÄ±za_90gÃ¼n_Flag'].sum():,} equipment flagged")

# Customer impact summary
customer_cols_found = [col for col in customer_impact_cols if any(col.replace(" ", "_") in c for c in equipment_df.columns)]
if customer_cols_found:
    print(f"\nğŸ‘¥ CUSTOMER IMPACT COLUMNS:")
    for col in customer_cols_found[:5]:  # Show first 5
        print(f"   âœ“ {col}")
    if len(customer_cols_found) > 5:
        print(f"   ... and {len(customer_cols_found)-5} more")

# Optional specifications summary
optional_cols_found = [col for col in optional_spec_cols.keys() if col in equipment_df.columns]
if optional_cols_found:
    print(f"\nğŸŒŸ OPTIONAL SPECIFICATIONS INCLUDED:")
    for col in optional_cols_found:
        coverage = equipment_df[col].notna().sum()
        pct = coverage / len(equipment_df) * 100
        print(f"   âœ“ {col}: {coverage:,} ({pct:.1f}% coverage)")

print(f"\nâœ… ENHANCEMENTS IN v3.0:")
print(f"   âœ¨ Day-precision age calculation (365.25 days/year)")
print(f"   âœ¨ Installation date preserved (Ekipman_Kurulum_Tarihi)")
print(f"   âœ¨ Age in days available (Ekipman_YaÅŸÄ±_GÃ¼n)")
if USE_FIRST_WORKORDER_FALLBACK:
    wo_count = (equipment_df['Age_Source'] == 'FIRST_WORKORDER_PROXY').sum()
    print(f"   âœ¨ First work order fallback ({wo_count} equipment)")
print(f"   âœ¨ Enhanced date validation with diagnostics")
print(f"   âœ¨ Vectorized operations for better performance")

print(f"\nğŸš€ READY FOR NEXT PHASE:")
print(f"   â†’ Run: 03_feature_engineering.py")
print(f"   â†’ Create advanced features (age ratios, reliability scores, etc.)")
print("="*100)
